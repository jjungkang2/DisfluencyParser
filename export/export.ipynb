{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file documents the process that was used to convert our model from a saved\n",
        "PyTorch checkpoint to a TensorFlow graph. The code here was run one cell at a\n",
        "time inside an IPython/Jupyter session.\n",
        "\n",
        "The ELMo code for TensorFlow can be found at:\n",
        "https://github.com/allenai/bilm-tf/commit/7cd16b0c1487587cadd4d5cffbb662e9013e990f\n",
        "with additional changes applied:\n",
        "0001-bilm-tf-changes-for-use-with-benepar.patch\n",
        "\"\"\"\n",
        "\n",
        "%cd ~/dev/self-attentive-parser\n",
        "import sys\n",
        "sys.path.insert(0, \"/Users/kitaev/dev/self-attentive-parser/src\")\n",
        "\n",
        "from bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import os.path\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.optim.lr_scheduler\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import evaluate\n",
        "import trees\n",
        "import vocabulary\n",
        "import nkutil\n",
        "import parse_nk\n",
        "tokens = parse_nk\n",
        "#%%\n",
        "\n",
        "def format_elapsed(start_time):\n",
        "    elapsed_time = int(time.time() - start_time)\n",
        "    minutes, seconds = divmod(elapsed_time, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    days, hours = divmod(hours, 24)\n",
        "    elapsed_string = \"{}h{:02}m{:02}s\".format(hours, minutes, seconds)\n",
        "    if days > 0:\n",
        "        elapsed_string = \"{}d{}\".format(days, elapsed_string)\n",
        "    return elapsed_string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "class args:\n",
        "    model_path_base=\"models/en_elmo_dev=95.21.pt\"\n",
        "    test_path=\"data/22.auto.clean\" # dev set\n",
        "    eval_batch_size=100\n",
        "    evalb_dir=\"EVALB/\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if True:\n",
        "    if parse_nk.use_cuda:\n",
        "        info = torch.load(args.model_path_base)\n",
        "    else:\n",
        "        info = torch.load(args.model_path_base, map_location=lambda storage, location: storage)\n",
        "    assert 'hparams' in info['spec'], \"Older savefiles not supported\"\n",
        "    parser = parse_nk.NKChartParser.from_spec(info['spec'], info['state_dict'])\n",
        "\n",
        "#%%\n",
        "\n",
        "print(\"Loading test trees from {}...\".format(args.test_path))\n",
        "test_treebank = trees.load_trees(args.test_path)\n",
        "print(\"Loaded {:,} test examples.\".format(len(test_treebank)))\n",
        "\n",
        "#%%\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "sd = parser.state_dict()\n",
        "\n",
        "LABEL_VOCAB = [x[0] for x in sorted(parser.label_vocab.indices.items(), key=lambda x: x[1])]\n",
        "LABEL_VOCAB\n",
        "\n",
        "#%%\n",
        "\n",
        "def make_elmo(chars_batched):\n",
        "    bilm = BidirectionalLanguageModel(\n",
        "                    options_file=\"data/elmo_2x4096_512_2048cnn_2xhighway_options.json\",\n",
        "                    weight_file=\"data/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\",\n",
        "                    max_batch_size=128)\n",
        "\n",
        "    lm = bilm(chars_batched)\n",
        "    word_representations_padded = weight_layers('scalar_mix', lm, l2_coef=0.0)['weighted_op']\n",
        "\n",
        "    # Strip off multiplication by gamma. Our parser has gamma=1 because there is a\n",
        "    # projection matrix right after\n",
        "    word_representations_padded = word_representations_padded.op.inputs[0]\n",
        "\n",
        "    with tf.variable_scope('', reuse=True):\n",
        "        elmo_scalar_mix_matrix = tf.get_variable('scalar_mix_ELMo_W')\n",
        "\n",
        "    tf.global_variables_initializer().run()\n",
        "    tf.assign(elmo_scalar_mix_matrix, [\n",
        "        float(sd['elmo.scalar_mix_0.scalar_parameters.0']),\n",
        "        float(sd['elmo.scalar_mix_0.scalar_parameters.1']),\n",
        "        float(sd['elmo.scalar_mix_0.scalar_parameters.2'])]).eval()\n",
        "\n",
        "    # Switch from padded to packed representation\n",
        "    valid_mask = lm['mask']\n",
        "    dim_padded = tf.shape(lm['mask'])[:2]\n",
        "    mask_flat = tf.reshape(lm['mask'], (-1,))\n",
        "    dim_flat = tf.shape(mask_flat)[:1]\n",
        "    nonpad_ids = tf.to_int32(tf.where(mask_flat)[:,0])\n",
        "    word_reps_shape = tf.shape(word_representations_padded)\n",
        "    word_representations_flat = tf.reshape(word_representations_padded, [-1, int(word_representations_padded.shape[-1])])\n",
        "    word_representations = tf.gather(word_representations_flat, nonpad_ids)\n",
        "\n",
        "    projected_annotations = tf.matmul(\n",
        "        word_representations,\n",
        "        tf.constant(sd['project_elmo.weight'].numpy().transpose()))\n",
        "\n",
        "    return projected_annotations, nonpad_ids, dim_flat, dim_padded, valid_mask, lm['lengths']\n",
        "\n",
        "#%%\n",
        "\n",
        "position_table = tf.constant(sd['embedding.position_table'], name=\"position_table\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def make_layer_norm(input, torch_name, name):\n",
        "    # TODO(nikita): The epsilon here isn't quite the same as in pytorch\n",
        "    # The pytorch code adds eps=1e-3 to the standard deviation, while this\n",
        "    # tensorflow code adds eps=1e-6 to the variance.\n",
        "    # However, the resulting mismatch in floating-point values does not seem to\n",
        "    # translate to any noticable changes in the parser's tree output\n",
        "    mean, variance = tf.nn.moments(input, [1], keep_dims=True)\n",
        "    return tf.nn.batch_normalization(\n",
        "        input,\n",
        "        mean, variance,\n",
        "        offset=tf.constant(sd[f'{torch_name}.b_2'], name=f\"{name}/offset\"),\n",
        "        scale=tf.constant(sd[f'{torch_name}.a_2'], name=f\"{name}/scale\"),\n",
        "        variance_epsilon=1e-6)\n",
        "\n",
        "\n",
        "def make_heads(input, shape_bthf, shape_xtf, torch_name, name):\n",
        "    res = tf.matmul(input,\n",
        "        tf.constant(sd[torch_name].numpy().transpose((1,0,2)).reshape((512, -1)), name=f\"{name}/W\"))\n",
        "    res = tf.reshape(res, shape_bthf)\n",
        "    res = tf.transpose(res, (0,2,1,3)) # batch x num_heads x time x feat\n",
        "    res = tf.reshape(res, shape_xtf) # _ x time x feat\n",
        "    return res\n",
        "\n",
        "def make_attention(input, nonpad_ids, dim_flat, dim_padded, valid_mask, torch_name, name):\n",
        "    input_flat = tf.scatter_nd(indices=nonpad_ids[:, None], updates=input, shape=tf.concat([dim_flat, tf.shape(input)[1:]], axis=0))\n",
        "    input_flat_dat, input_flat_pos = tf.split(input_flat, 2, axis=-1)\n",
        "\n",
        "    shape_bthf = tf.concat([dim_padded, [8, -1]], axis=0)\n",
        "    shape_bhtf = tf.convert_to_tensor([dim_padded[0], 8, dim_padded[1], -1])\n",
        "    shape_xtf = tf.convert_to_tensor([dim_padded[0] * 8, dim_padded[1], -1])\n",
        "    shape_xf = tf.concat([dim_flat, [-1]], axis=0)\n",
        "\n",
        "    qs1 = make_heads(input_flat_dat, shape_bthf, shape_xtf, f'{torch_name}.w_qs1', f'{name}/q_dat')\n",
        "    ks1 = make_heads(input_flat_dat, shape_bthf, shape_xtf, f'{torch_name}.w_ks1', f'{name}/k_dat')\n",
        "    vs1 = make_heads(input_flat_dat, shape_bthf, shape_xtf, f'{torch_name}.w_vs1', f'{name}/v_dat')\n",
        "    qs2 = make_heads(input_flat_pos, shape_bthf, shape_xtf, f'{torch_name}.w_qs2', f'{name}/q_pos')\n",
        "    ks2 = make_heads(input_flat_pos, shape_bthf, shape_xtf, f'{torch_name}.w_ks2', f'{name}/k_pos')\n",
        "    vs2 = make_heads(input_flat_pos, shape_bthf, shape_xtf, f'{torch_name}.w_vs2', f'{name}/v_pos')\n",
        "\n",
        "    qs = tf.concat([qs1, qs2], axis=-1)\n",
        "    ks = tf.concat([ks1, ks2], axis=-1)\n",
        "    attn_logits = tf.matmul(qs, ks, transpose_b=True) / (1024 ** 0.5)\n",
        "\n",
        "    attn_mask = tf.reshape(tf.tile(valid_mask, [1,8*dim_padded[1]]), tf.shape(attn_logits))\n",
        "    # TODO(nikita): use tf.where and -float('inf') here?\n",
        "    attn_logits -= 1e10 * tf.to_float(~attn_mask)\n",
        "\n",
        "    attn = tf.nn.softmax(attn_logits)\n",
        "\n",
        "    attended_dat_raw = tf.matmul(attn, vs1)\n",
        "    attended_dat_flat = tf.reshape(tf.transpose(tf.reshape(attended_dat_raw, shape_bhtf), (0,2,1,3)), shape_xf)\n",
        "    attended_dat = tf.gather(attended_dat_flat, nonpad_ids)\n",
        "    attended_pos_raw = tf.matmul(attn, vs2)\n",
        "    attended_pos_flat = tf.reshape(tf.transpose(tf.reshape(attended_pos_raw, shape_bhtf), (0,2,1,3)), shape_xf)\n",
        "    attended_pos = tf.gather(attended_pos_flat, nonpad_ids)\n",
        "\n",
        "    out_dat = tf.matmul(attended_dat, tf.constant(sd[f'{torch_name}.proj1.weight'].numpy().transpose()))\n",
        "    out_pos = tf.matmul(attended_pos, tf.constant(sd[f'{torch_name}.proj2.weight'].numpy().transpose()))\n",
        "\n",
        "    out = tf.concat([out_dat, out_pos], -1)\n",
        "    return make_layer_norm(input + out, f'{torch_name}.layer_norm', f'{name}/layer_norm')\n",
        "\n",
        "def make_dense_relu_dense(input, torch_name, torch_type, name):\n",
        "    # TODO: use name\n",
        "    mul1 = tf.matmul(input, tf.constant(sd[f'{torch_name}.w_1{torch_type}.weight'].numpy().transpose()))\n",
        "    mul1b = tf.nn.bias_add(mul1, tf.constant(sd[f'{torch_name}.w_1{torch_type}.bias']))\n",
        "    mul1b = tf.nn.relu(mul1b)\n",
        "    mul2 = tf.matmul(mul1b, tf.constant(sd[f'{torch_name}.w_2{torch_type}.weight'].numpy().transpose()))\n",
        "    mul2b = tf.nn.bias_add(mul2, tf.constant(sd[f'{torch_name}.w_2{torch_type}.bias']))\n",
        "    return mul2b\n",
        "\n",
        "def make_ff(input, torch_name, name):\n",
        "    # TODO: use name\n",
        "    input_dat, input_pos = tf.split(input, 2, axis=-1)\n",
        "    out_dat = make_dense_relu_dense(input_dat, torch_name, 'c', name=\"TODO_dat\")\n",
        "    out_pos = make_dense_relu_dense(input_pos, torch_name, 'p', name=\"TODO_pos\")\n",
        "    out = tf.concat([out_dat, out_pos], -1)\n",
        "    return make_layer_norm(input + out, f'{torch_name}.layer_norm', f'{name}/layer_norm')\n",
        "\n",
        "def make_stacks(input, nonpad_ids, dim_flat, dim_padded, valid_mask, num_stacks):\n",
        "    res = input\n",
        "    for i in range(num_stacks):\n",
        "        res = make_attention(res, nonpad_ids, dim_flat, dim_padded, valid_mask, f'encoder.attn_{i}', name=f'attn_{i}')\n",
        "        res = make_ff(res, f'encoder.ff_{i}', name=f'ff_{i}')\n",
        "    return res\n",
        "\n",
        "def make_layer_norm_with_constants(input, constants):\n",
        "    # TODO(nikita): The epsilon here isn't quite the same as in pytorch\n",
        "    # The pytorch code adds eps=1e-3 to the standard deviation, while this\n",
        "    # tensorflow code adds eps=1e-6 to the variance.\n",
        "    # However, the resulting mismatch in floating-point values does not seem to\n",
        "    # translate to any noticable changes in the parser's tree output\n",
        "    mean, variance = tf.nn.moments(input, [1], keep_dims=True)\n",
        "    return tf.nn.batch_normalization(\n",
        "        input,\n",
        "        mean, variance,\n",
        "        offset=constants[0],\n",
        "        scale=constants[1],\n",
        "        variance_epsilon=1e-6)\n",
        "\n",
        "def make_flabel_with_constants(input, constants):\n",
        "    mul1 = tf.matmul(input, constants[0])\n",
        "    mul1b = tf.nn.bias_add(mul1, constants[1])\n",
        "    mul1b = make_layer_norm_with_constants(mul1b, constants[2:4])\n",
        "    mul1b = tf.nn.relu(mul1b)\n",
        "    mul2 = tf.matmul(mul1b, constants[4])\n",
        "    mul2b = tf.nn.bias_add(mul2, constants[5], name='flabel')\n",
        "    return mul2b\n",
        "\n",
        "\n",
        "def make_flabel_constants():\n",
        "    return (\n",
        "        tf.constant(sd['f_label.0.weight'].numpy().transpose()),\n",
        "        tf.constant(sd['f_label.0.bias']),\n",
        "        tf.constant(sd['f_label.1.b_2'], name=\"label/layer_norm/offset\"),\n",
        "        tf.constant(sd['f_label.1.a_2'], name=\"label/layer_norm/scale\"),\n",
        "        tf.constant(sd['f_label.3.weight'].numpy().transpose()),\n",
        "        tf.constant(sd['f_label.3.bias']),\n",
        "    )\n",
        "\n",
        "def make_network():\n",
        "    # batch x num_words x 50\n",
        "    chars = tf.placeholder(shape=(None, None, 50), dtype=tf.int32, name='chars')\n",
        "\n",
        "    input_dat, nonpad_ids, dim_flat, dim_padded, valid_mask, lengths = make_elmo(chars)\n",
        "    chars_shape = tf.shape(chars)\n",
        "    input_pos_flat = tf.tile(position_table[:chars_shape[1]], [chars_shape[0], 1])\n",
        "    input_pos = tf.gather(input_pos_flat, nonpad_ids)\n",
        "\n",
        "    input_joint = tf.concat([input_dat, input_pos], -1)\n",
        "    input_joint = make_layer_norm(input_joint, 'embedding.layer_norm', 'embedding/layer_norm')\n",
        "\n",
        "    word_out = make_stacks(input_joint, nonpad_ids, dim_flat, dim_padded, valid_mask, num_stacks=4)\n",
        "    word_out = tf.concat([word_out[:, 0::2], word_out[:, 1::2]], -1)\n",
        "\n",
        "\n",
        "    fp_out = tf.concat([word_out[:-1,:512], word_out[1:,512:]], -1)\n",
        "\n",
        "    fp_start_idxs = tf.cumsum(lengths, exclusive=True)\n",
        "    fp_end_idxs = tf.cumsum(lengths) - 1 # the number of fenceposts is 1 less than the number of words\n",
        "\n",
        "    fp_end_idxs_uneven = fp_end_idxs - tf.convert_to_tensor([1, 0])\n",
        "\n",
        "    # Have to make these outside tf.map_fn for model compression to work\n",
        "    constants = make_flabel_constants()\n",
        "\n",
        "    def to_map(start_and_end):\n",
        "        start, end = start_and_end\n",
        "        fp = fp_out[start:end]\n",
        "        # flabel = make_flabel(tf.reshape(fp[None,:,:] - fp[:,None,:], (-1, 1024)))\n",
        "        flabel = make_flabel_with_constants(tf.reshape(fp[None,:,:] - fp[:,None,:], (-1, 1024)), constants)\n",
        "        actual_chart_size = end-start\n",
        "        flabel = tf.reshape(flabel, [actual_chart_size, actual_chart_size, -1])\n",
        "        amount_to_pad = dim_padded[1] - actual_chart_size\n",
        "        # extra padding on the label dimension is for the not-a-constituent label,\n",
        "        # which always has a score of 0\n",
        "        flabel = tf.pad(flabel, [[0, amount_to_pad], [0, amount_to_pad], [1, 0]])\n",
        "        return flabel\n",
        "\n",
        "    charts = tf.map_fn(to_map, (fp_start_idxs, fp_end_idxs), dtype=(tf.float32))\n",
        "    charts = tf.identity(charts, name=\"charts\")\n",
        "\n",
        "    return chars, charts"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def charify_batch(sentences):\n",
        "    ELMO_START_SENTENCE = 256\n",
        "    ELMO_STOP_SENTENCE = 257\n",
        "    ELMO_START_WORD = 258\n",
        "    ELMO_STOP_WORD = 259\n",
        "    ELMO_CHAR_PAD = 260\n",
        "\n",
        "    padded_len = max([len(sentence) + 2 for sentence in sentences])\n",
        "\n",
        "    all_chars = np.zeros((len(sentences), padded_len, 50), dtype=np.int32)\n",
        "\n",
        "    for snum, sentence in enumerate(sentences):\n",
        "        all_chars[snum, :len(sentence)+2,:] = ELMO_CHAR_PAD\n",
        "\n",
        "        all_chars[snum, 0, 0] = ELMO_START_WORD\n",
        "        all_chars[snum, 0, 1] = ELMO_START_SENTENCE\n",
        "        all_chars[snum, 0, 2] = ELMO_STOP_WORD\n",
        "\n",
        "        for i, word in enumerate(sentence):\n",
        "            chars = [ELMO_START_WORD] + list(word.encode('utf-8', 'ignore')[:(50-2)]) + [ELMO_STOP_WORD]\n",
        "            all_chars[snum, i+1, :len(chars)] = chars\n",
        "\n",
        "        all_chars[snum, len(sentence)+1, 0] = ELMO_START_WORD\n",
        "        all_chars[snum, len(sentence)+1, 1] = ELMO_STOP_SENTENCE\n",
        "        all_chars[snum, len(sentence)+1, 2] = ELMO_STOP_WORD\n",
        "\n",
        "        # Add 1; 0 is a reserved value for signaling words past the end of the\n",
        "        # sentence, which we don't have because batch_size=1\n",
        "        all_chars[snum, :len(sentence)+2,:] += 1\n",
        "\n",
        "    return all_chars"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "the_inp, the_out = make_network()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def tf_parse_batch(sentences):\n",
        "    inp_val = charify_batch([[word for (tag, word) in sentence] for sentence in sentences])\n",
        "    out_val = sess.run(the_out, {the_inp: inp_val})\n",
        "\n",
        "    trees = []\n",
        "    scores = []\n",
        "    for snum, sentence in enumerate(sentences):\n",
        "        chart_size = len(sentence) + 1\n",
        "        tf_chart = out_val[snum,:chart_size,:chart_size,:]\n",
        "        tree, score = parser.decode_from_chart(sentence, tf_chart)\n",
        "        trees.append(tree)\n",
        "        scores.append(score)\n",
        "    return trees, scores\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "print(\"Parsing test sentences using tensorflow...\")\n",
        "start_time = time.time()\n",
        "\n",
        "test_predicted = []\n",
        "for start_index in range(0, len(test_treebank), args.eval_batch_size):\n",
        "# for start_index in range(0, 2, 2):\n",
        "    print(start_index, format_elapsed(start_time))\n",
        "    subbatch_trees = test_treebank[start_index:start_index+args.eval_batch_size]\n",
        "    subbatch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in subbatch_trees]\n",
        "    predicted, _ = tf_parse_batch(subbatch_sentences)\n",
        "    del _\n",
        "    test_predicted.extend([p.convert() for p in predicted])\n",
        "\n",
        "test_fscore = evaluate.evalb(args.evalb_dir, test_treebank[:len(test_predicted)], test_predicted)\n",
        "\n",
        "print('Done', format_elapsed(start_time))\n",
        "str(test_fscore)\n",
        "\n",
        "#%%\n",
        "\n",
        "input_node_names = [the_inp.name.split(':')[0]]\n",
        "output_node_names = [the_out.name.split(':')[0]]\n",
        "\n",
        "print(\"Input node names:\", input_node_names)\n",
        "print(\"Output node names:\", output_node_names)\n",
        "\n",
        "graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names)\n",
        "\n",
        "#%%\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "graph_def = TransformGraph(graph_def, input_node_names, output_node_names, [\n",
        "'strip_unused_nodes()',\n",
        "'remove_nodes(op=Identity, op=CheckNumerics)',\n",
        "'fold_constants()',\n",
        "'fold_old_batch_norms',\n",
        "'fold_batch_norms',\n",
        "'round_weights(num_steps=128)',\n",
        "])\n",
        "\n",
        "#%%\n",
        "\n",
        "with open('batched_elmo128.pb', 'wb') as f:\n",
        "    f.write(graph_def.SerializeToString())\n",
        "\n",
        "#%%\n",
        "newg = tf.Graph()\n",
        "\n",
        "with newg.as_default():\n",
        "    tf.import_graph_def(graph_def)\n",
        "\n",
        "new_inp = newg.get_tensor_by_name('import/chars:0')\n",
        "new_out = newg.get_tensor_by_name('import/charts:0')\n",
        "\n",
        "new_sess = tf.InteractiveSession(graph=newg)\n",
        "#%%\n",
        "\n",
        "def tf_parse_batch_new(sentences):\n",
        "    inp_val = charify_batch([[word for (tag, word) in sentence] for sentence in sentences])\n",
        "    out_val = new_sess.run(new_out, {new_inp: inp_val})\n",
        "\n",
        "    trees = []\n",
        "    scores = []\n",
        "    for snum, sentence in enumerate(sentences):\n",
        "        chart_size = len(sentence) + 1\n",
        "        tf_chart = out_val[snum,:chart_size,:chart_size,:]\n",
        "        tree, score = parser.decode_from_chart(sentence, tf_chart)\n",
        "        trees.append(tree)\n",
        "        scores.append(score)\n",
        "    return trees, scores\n",
        "\n",
        "print(\"Parsing test sentences using tensorflow...\")\n",
        "start_time = time.time()\n",
        "\n",
        "test_predicted = []\n",
        "for start_index in range(0, len(test_treebank), args.eval_batch_size):\n",
        "    subbatch_trees = test_treebank[start_index:start_index+args.eval_batch_size]\n",
        "    subbatch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in subbatch_trees]\n",
        "    predicted, _ = tf_parse_batch_new(subbatch_sentences)\n",
        "    del _\n",
        "    test_predicted.extend([p.convert() for p in predicted])\n",
        "\n",
        "test_fscore = evaluate.evalb(args.evalb_dir, test_treebank[:len(test_predicted)], test_predicted)\n",
        "\n",
        "str(test_fscore)\n",
        "\n",
        "#%%\n",
        "#%%\n",
        "#%%\n",
        "#%%\n",
        "#%%\n",
        "#%%\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}